\documentclass[a4paper]{article}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatimbox}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[compact]{titlesec}
\usepackage[bottom=1in]{geometry}


\usepackage{booktabs}
\usepackage{tabu}


\usepackage{titling}
\setlength{\droptitle}{-10em}   % This is your set screw

\usepackage{ntheorem}
\theorembodyfont{\normalfont}
\newtheorem{mytheorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{axiom}{Axiom}
\newtheorem{mydef}{Definition}
\numberwithin{mytheorem}{section}
\numberwithin{mydef}{section}
\numberwithin{example}{section}

\title{\textbf{Linear algebra notes (based on Friedberg's text)}}
\author{Abiyaz Chowdhury }
\date{\today}

\parindent 0in
\setlength{\parskip}{0.5em}

\newcommand{\done}{$\blacksquare$ }

\begin{document}
\maketitle

\section{Vector Spaces}

\subsection{Vector Spaces}

\begin{mydef} A vector space (or linear space) $V$ over a field $F$ consists of a set on which two operations (called addition and scalar multiplication, respectively) are defined so that for each pair of elements $x,y$ in $V$ there is a unique element $x + y$ in $V$, and for each element $a$ in $F$, and each element $x \in V$ there  is a unique element $ax$ in $V$, such that the following conditions hold:
\begin{enumerate}
\item For all $x,y \in V$, $x + y = y + x$. (commutativity of addition)
\item For all $x,y,z \in V$, $(x + y) + z = x + (y+z)$. (associativity of addition)
\item There exists an element in $V$ denoted by $0$ such that $x + 0 = x$ for each $x \in V$. (identity under addition)
\item For each element $x \in V$ there is an element $y \in V$ such that $x + y = 0$. (existence of additive inverse)
\item For each element $x \in V$, $1x = x$. (identity under scalar multiplication)
\item For each pair of elements $a,b \in F$ and  each element $x \in V$, $(ab)x = a(bx)$. (associativity under scalar multiplication) 
\item For each element $a \in F$ and each pair of elements $x,y \in V$, $a(x + y) = ax + ay$. (scalar multiplication distributes over addition)
\item For each pair of elements $a, b \in F$ and each element $x \in V$, $(a+b)x = ax + bx$. (scalar multiplication distributes over addition)
\end{enumerate}
The elements $x+y$ and $ax$ are called the sum of $x$ and $y$ and the product of $a$ and $x$, respectively. The elements of the field $F$ are called scalars and the elements of the vector space $V$ are called vectors. The element $0$ as defined in the third item is called the zero vector of $V$.
\end{mydef}

\begin{mydef} An object of the form $(a_{1},a_{2},...,a_{n})$ where the entries $a_{1},a_{2},...,a_{n}$ are elements of a field $F$, is called an n-tuple with entries from $F$. The elements $a_{1},a_{2},...,a_{n}$  are called the entries or components of the n-tuple. Two n-tuples $(a_{1},a_{2}...a_{n})$ and $(b_{1},b_{2},..,b_{n})$ with entries from a field $F$ are called equal if $a_{i} = b_{i}$ for all $i = 1,2...,n$. 
\end{mydef}

\begin{mydef} The set of all n-tuples with entries from a field $F$ is denoted by $F^{n}$.
\end{mydef}

\begin{mydef} The sum of two n-tuples $(a_{1},a_{2}...a_{n})$ and $(b_{1},b_{2},..,b_{n})$ in $F^{n}$ yields the n-tuple $(c_{1},c_{2},..,c_{n}) \in F^{n}$ where $c_{i} = a_{i}+b_{i}$ for all $i = 1,2,..,n$.  
\end{mydef}

\begin{mydef} The scalar product of an n-tuple $(a_{1},a_{2}...a_{n})$ in $F^{n}$ and a scalar $c \in F$ yields the n-tuple $(c_{1},c_{2},..,c_{n}) \in F^{n}$ where $c_{i} = c[a_{i}] $ for all $i = 1,2,..,n$.  
\end{mydef}

\begin{mytheorem} $F^{n}$, under the operations defined above, is a vector space over $F$, with zero vector $(0,0,..0)$ where $0$ is the additive identity of $F$. \end{mytheorem}

\begin{mydef} An $m \times n$ matrix with entries from a field $F$ is a rectangular array of the form 
\[
\begin{bmatrix}
    a_{11}       & a_{12}  & \dots & a_{1n} \\
    a_{21}       & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}       & a_{m2} & \dots & a_{mn}
\end{bmatrix}
\] 
where each entry $a_{ij} (1 \leq i \leq m, 1 \leq j \leq n)$ is an element of $F$. We  call the entries $a_{ij}$ with $i = j$ the diagonal entries of the matrix. The entries $a_{i1},a_{2j},...a_{mj}$ compose the jth column of the matrix. The rows of the preceding matrix are regarded as vectors in $F^{n}$, and columns are regarded as vectors in $F^{m}$. The $m \times n$ matrix in which each entry equals zero is called the zero matrix and is denoted by $O$. 
\end{mydef}

\begin{mydef} In the preceding matrix, $m$ is the number of rows and $n$ is the number of columns of the matrix. The entry of matrix $A$ that lieds in row $i$ and column $j$ is denoted by $A_{ij}$. If a matrix has an equal number of rows and columns, it is called square. Two $m \times n$ matrices $A$ and $B$ are called equal if all their corresponding entries are equal, that is, if $A_{ij} = B_{ij}$ for $1 \leq i \leq m$ and $1 \leq j \leq n$. 
\end{mydef}

\begin{mydef} An $m \times n$ matrix $A$ where $A_{ij} = 0$ whenever $ i \neq j$ is called a diagonal matrix.
\end{mydef}

\begin{mydef} An $m \times n$ matrix $A$ where $A_{ij} = 0$ whenever $ i > j$ is called an upper triangular matrix.
\end{mydef}

\begin{mydef} An $m \times n$ matrix $A$ where $A_{ij} = 0$ whenever $ i < j$ is called a lower triangular matrix.
\end{mydef}

\begin{mydef} The set of all $m \times n$ matrices with entries from a field $F$ is denoted by $M_{m \times n}(F)$.  
\end{mydef}

\begin{mydef} Addition of two $m \times n$ matrices $A$ and $B$ yields the $m \times n$ matrix $C$ where $C_{ij} = A_{ij} + B_{ij}$.  
\end{mydef}

\begin{mydef} Scalar Multiplication of an $m \times n$ matrix $A$ with entries from $F$ and a scalar $c \in F$ yields an $m \times n$ matrix $C$ with entries from $F$ where $C_{ij} = cA_{ij}$. 
\end{mydef}

\begin{mytheorem} $M_{m \times n}(F)$, under the operations defined above, is a vector space over $F$, with zero vector $A$ where $A_{ij} = 0$ for all $i = 1,2,..,m$ and $j = 1,2,...,n$ and $0$ is the additive identity of $F$. \end{mytheorem}

\begin{mydef} Matrix multiplication of an $m \times n$ matrix $A$ and a $n \times p$ matrix $B$ yields the $m \times p$ matrix $C$ where $C_{ij} = \sum^{n}_{k = 1} A_{ik}B_{kj}$.  
\end{mydef}

\begin{mydef} Let $S$ be any nonempty set and $F$ be any field, and let $\mathcal{F}(S,F)$ denote the set of all functions from $S$ to $F$. Two functions $f$ and $g$ in $\mathcal{F}(S,F)$ are called equal if $f(s) = g(s)$ for each $s \in S$. We define the sum of two functions $f,g \in \mathcal{F}(S,F)$ as the function $h \in \mathcal{F}(S,F) $ where $h(s) = f(s) + g(s)$. We define the scalar multiplication of a function $f \in \mathcal{F}(S,F) $ by a scalar $c \in F$ as the function $h \in \mathcal{F}(S,F) $ where $h(s) = c[f(s)]$.  
\end{mydef}

\begin{mytheorem} $\mathcal{F}(S,F)$ is a vector space, under the operations defined above, with zero vector $f \in \mathcal{F}(S,F)$ where $f(s) = 0$ for all $s \in 0$ is the zero vector in $\mathcal{F}(S,F)$.   \end{mytheorem}

\begin{mydef} A polynomial with coefficients from a field $F$ is an expression of the form $f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + ... + a_{1}x + a_{0}$ where $n$ is a nonnegative integer and each $a_{k}$, called the coefficient of $x^{k}$, is in $F$. If $f(x) = 0$, that is, if $a_{n} = a_{n-1} = ... = a_{0} = 0$, then $f(x)$ is called the zero polynomial, and the degree is defined as $-1$. Otherwise, the degree is defined as the largest exponent of $x$ that appears in the representation $f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + ... + a_{1}x + a_{0}$ with a nonzero coefficient. A nonzero scalar in $F$ therefore is a polynomial of degree $0$. Two polynomials $f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + ... + a_{1}x + a_{0}$ and $g(x) = b_{n}x^{n} + b_{n-1}x^{n-1} + ... + b_{1}x + b_{0}$ are called equal if $m = n$ and $a_{i} = b_{i}$ for $i = 0,1,..,n$. A polynomial $f$ can be evaluated at a point $x_{0}$ by replacing in the polynomial every occurrence of the polynomial's variable with $x_{0}$. The result thus obtained is an element of $F$, and is written as $f(x_{0})$.
\end{mydef}

\begin{example}
The sum of two polynomials $f,g \in \mathcal{F}(S,F)$ having degrees $m,n$ respectively 
\end{example}

\begin{mydef} A sequence in $F$ is a function $\sigma$ from the positive integers into $F$. A sequence is typically represented as an infinite series of numbers, i.e.  $a_{1},a_{2},...$ where the entries are elements in $F$. We can represent this sequence as simply $\{ a_{n} \}$. The sum of two sequences $\{ a_{n} \}$ and $\{ b_{n} \}$ yields a sequence $\{ c_{n} \}$ in $F$ where $c_{i} = a_{i} + b_{i}$ and the scalar product of a sequence $\{ a_{n} \}$ in $F$ and a scalar $c \in F$ yields a sequence $\{ c_{n} \}$ in $F$ where $c_{i} = c[a_{i}]$.  
\end{mydef}

\begin{mytheorem} The set of all sequences in $F$, forms a vector space over $F$ under the operations defined above.  \end{mytheorem}

\begin{mytheorem} If $V$ is a vector space and $x,y,z \in V$ such that $x + z = y + z$, then $x = y$. \end{mytheorem}

\begin{mytheorem} The zero vector of any vector space $V$ is unique. \end{mytheorem}

\begin{mytheorem} The additive identity of any vector is unique. We typically denote the additive identity of $x$ by $-x$. \end{mytheorem}

\begin{mytheorem} For a vector space $V$, the following are true:
\begin{enumerate}
\item $0x = 0 $ for all $x \in V$.
\item $(-a)x = -(ax) = a(-x)$ for all $a \in F$ and each $x \in V$.
\item $a0 = 0$ for each $a \in F$. 
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} For a vector space $V$, the following are true:
\begin{enumerate}
\item $0x = 0 $ for all $x \in V$.
\item $(-a)x = -(ax) = a(-x)$ for all $a \in F$ and each $x \in V$.
\item $a0 = 0$ for each $a \in F$. 
\end{enumerate}
\end{mytheorem}

\begin{example}
Define the set $V = \{ 0 \}$ where $0 + 0 = 0$ and $c0 = 0$ for all $c \in F$. Then V is called the zero vector space. 
\end{example}

\subsection{Subspaces}

\begin{mydef} A subset $W$ of a vector space $V$ over a field  $F$ is called a subspace of $V$ if $W$ is also a vector space over $F$ with the same operations of addition and scalar multiplication as defined on $V$.
\end{mydef}

\begin{example}
For any vector space $V$, note that $V$ and $\{ 0 \}$ are subspaces of $V$, where $0$ is the zero vector of $V$.
\end{example}

\begin{mytheorem} For a vector space $V$, a subset $W$ of $V$ is a subspace of $V$ if and only if the following are all true:
\begin{enumerate}
\item $x + y \in W$ if $x \in W$ and $y \in W$.
\item $cx \in W$ if $x \in W$ and $c \in F$. 
\item 
\end{enumerate}
\end{mytheorem}

\begin{mydef} The transpose $A^{t}$ of an $m \times n$ matrix $A$ is the $n \times m$ matrix defined by $A^{t}_{ij} = A_{ji}$. 
\end{mydef}

\begin{mydef} A symmetric matrix is one that equals its own transpose. Note that symmetric matrices are always square.
\end{mydef}

\begin{example} The set of all symmetric $n \times n$ matrices is a subspace of $M_{n \times n}(F)$. 
\end{example}

\begin{example} The set of all diagonal $n \times n$ matrices is a subspace of $M_{n \times n}(F)$ . 
\end{example}

\begin{example} The set of all upper triangular $m \times n$ matrices as well as the set of all lower triangular $m \times n$ matrices are both subspaces of $M_{m\times n}(F)$ . 
\end{example}

\begin{mydef} The trace of an $n \times n$ matrix $M$, denoted by $\text{tr}(M)$, is the sum of the diagonal entries of $M$, i.e. $$\text{tr}(M) = \sum^{n}_{i = 1}M_{ii}$$. 
\end{mydef}

\begin{example} The set of all $n \times n$ matrices having trace equal to 0 is a subspace of $M_{n \times n}(F)$. 
\end{example}

\begin{mytheorem} Any intersection of subspaces of a vector space $V$ is also a subspace of $V$.
\end{mytheorem}

\begin{mydef} If $W_{1}$ and $W_{2}$ are both subspaces of $V$, then $W_{1} + W_{2}$ is defined as the set $\{ w_{1} + w_{2} | w_{1} \in W_{1}, w_{2} \in W_{2} \}$. This set is called the set sum or simply the sum of $W_{1},W_{2}$. If it is also true that $W_{1} \cap W_{2} = \{ 0\}$, then $W_{1} + W_{2}$ is called the direct sum of $W_{1}$ and $W_{2}$ and we use the stricter notation $W_{1} \bigoplus W_{2}$ . Note that the direct sum is only defined for subspaces whose intersection is only at the zero vector but the set sum is defined for any two subspaces of the same vector space.
\end{mydef}

\begin{mytheorem}  If $W_{1}$ and $W_{2}$ are both subspaces of $V$, then $W_{1} \bigoplus W_{2}$ is the direct sum of $W_{1}$ and $W_{2}$ if and only if every vector in $W_{1} \bigoplus W_{2}$ can be uniquely represented as the sum of a vector in $W_{1}$ and a vector in $W_{2}$. 
\end{mytheorem}

\begin{example} A matrix $M$ is skew-symmetric if $M^{t} = -M$. A skew-symmetric matrix must be square. The set of all skew-symmetric $n \times n$ matrices is a subspace of $M_{n \times n}(F)$. 
\end{example}

\begin{example} The direct sum of the set of all skew-symmetric $n \times n$ matrices and the set of all symmetric $n \times n$ matrices is $M_{n \times n}(F)$.
\end{example}

\begin{mydef} If $W$ is a subspace of $V$ over $F$, then for $v \in V$, the set $v + W = \{ v + w | w \in W \}$ is called the coset of $W$ containing $V$. 
\end{mydef}

\begin{example} The coset $v + W$ is a subspace of $V$ if and only if $v \in W$.
\end{example}

\begin{example} If $v_{1},v_{2} \in V$, then $v_{1} + W = v_{2} = W$ if and only if $v_{1} - v_{2} \in W$.
\end{example}

\begin{mydef} The sum of two cosets $v_{1} + W$ and $v_{2} + W$ is defined as $(v_{1} + v_{2}) + W$. Likewise, the multiplication of a coset $v + W$ by a scalar $c \in F$ is defined as $cv + W$.
\end{mydef}

\begin{example} The collection of all cosets of $W$ with the operations defined above is a vector space, known as the quotient space of $V$ modulo $W$ and is denoted by $V/W$.
\end{example}

\subsection{Linear combinations and systems of linear equations}

\begin{mytheorem} If $V$ is a vector space over $F$, and $v_{1},...,v_{n} \in V$ and $c_{1},...,c_{n} \in F$, then $\sum^{n}_{i = 1}c_{i}v_{i}$ is in $V$ and is called a linear combination of the vectors $v_{1},...,v_{n}$. The scalars $c_{1},...,c_{n}$ are called the coefficients of the linear combination.
\end{mytheorem}

\begin{mydef} If $S$ is a nonempty subset of a vector space $V$, then we define the span of $S$, denoted $span(S)$ as the set of all linear combinations of the vectors in $S$. We define $span(\varnothing) = \{ 0 \}$.
\end{mydef}

\begin{mytheorem} If $S$ is a nonempty subset of a vector space $V$, $v \in span(S)$ if and only if $v$ can be represented as a linear combination of finitely many vectors in $S$.
\end{mytheorem}

\begin{mytheorem} If $S$ is a nonempty subset of a vector space $V$, then $span(S)$ is equivalent to the intersection of all subspaces of $V$ that contain $S$.
\end{mytheorem}

\begin{mytheorem} If $S$ is a nonempty subset of a vector space $V$, then $span(S)$ is a subspace of $V$. Furthermore, any subspace that contains $S$ must contain $span(S)$.
\end{mytheorem}

\begin{mydef} A subset $S$ of a vector space $V$ generates (or spans) $V$ if $span(S) = V$. 
\end{mydef}

\begin{example} If $W$ is a subspace of a vector space $V$, then $W$ is a subspace of $V$ if and only if $span(W) = W$.
\end{example}

\begin{example} (Idempotence of span) For any subset $S$ of a vector space $V$, $span(span(S)) = span(S)$.
\end{example}

\begin{example} If $S_{1},S_{2} \subseteq V$, and $S_{1} \subseteq S_{2}$, then $span(S_{1}) \subseteq span(S_{2})$. In particular, if $S_{1} \subseteq S_{2}$, and $span(S_{1}) = V$, then $span(S_{2}) = V$.
\end{example}

\begin{example} If $S_{1},S_{2} \subseteq V$, and $S_{1} \subseteq S_{2}$, then $span(S_{1}) \subseteq span(S_{2})$. In particular, if $S_{1} \subseteq S_{2}$, and $span(S_{1}) = V$, then $span(S_{2}) = V$.
\end{example}

\begin{example} If $S_{1}$ and $S_{2}$ are arbitrary subsets of a vector space $V$, then $span(S_{1} \cup S_{2}) = span(S_{1}) + span(S_{2})$.
\end{example}

\begin{example} If $S_{1}$ and $S_{2}$ are arbitrary subsets of a vector space $V$, then $span(S_{1} \cap S_{2}) \subseteq span(S_{1}) \cap span(S_{2})$.
\end{example}

\subsection{Linear dependence and linear independence}

\begin{mydef} A nonempty subset $S$ of a vector space $V$ is called linearly dependent (or just depdendent) if there exists a finite number of distinct vectors $u_{1},...,u_{n} \in S$ and scalars $a_{1},...,a_{n}$ not all zero, such that $a_{1}u_{1} + ... + a_{n}u_{n} = 0$. The vectors of $S$ are also said to be linearly dependent. A set that is not linearly dependent is linearly independent (or just independent). Note that the empty set is linearly independent, since dependent sets must be nonempty.
\end{mydef}

\begin{mytheorem} Sets containing the zero vector are dependent, and a singleton set consisting of a nonzero vector is independent. 
\end{mytheorem}

\begin{mytheorem} A subset $S$ of a vector space $V$ is independent if and only if every representation of $0 \in V$ as a linear combination of vectors in $S$ has all its coefficients equal to $0$.
\end{mytheorem}

\begin{mytheorem} A subset $S$ of a vector space $V$ is independent if and only if every finite subset of $S$ is independent.
\end{mytheorem}

\begin{mytheorem} A subset $S$ of a vector space $V$ is dependent if and only if at least one of its vectors can be represented as a linear combination of finitely many other vectors in $S$.
\end{mytheorem}

\begin{mytheorem}Let $V$ be a vector space, and let $S_{1} \subseteq S_{2} \subseteq V$. Then if $S_{1}$ is dependent, so is $S_{2}$. Stating the contrapositive, if $S_{2}$ is independent, so is $S_{1}$.
\end{mytheorem}

\begin{mytheorem}Let $S$ be an independent subset of a vector space $V$ and let $v$ be a vector not in $S$. Then $S \cup \{ v \}$ is dependent if and only if $v \in span(S)$.
\end{mytheorem}


\subsection{Bases and dimension}

\begin{mydef} A basis $\beta$ for a vector space $V$ is a linearly independent subset of $V$ that generates $V$. If $\beta$ is a basis for $V$, we also say that the vectors of $\beta$ form a basis for $V$.
\end{mydef}

\begin{example} In $F^{n}$, let $e_{1} = (1,0,...,0), e_{2} = (0,1,0,...,0), ..., e_{n} = (0,...,0,1)$. Then the set $\{ e_{1},e_{2},...,e_{n} \}$ is a basis for $F^{n}$ and is called the standard basis for $F^{n}$.
\end{example}

\begin{example} The set $\{ 1,x,x^{2},..., \} $ is a basis for the set of all polynomials in $x$.
\end{example}

\begin{mytheorem} Let $V$ be a vector space and $\beta = \{ u_{1},u_{2},...,u_{n} \} $ be a subset of $V$. Then $\beta$ is a basis for $V$ if and only if each $v \in V$ can be uniquely expressed as a linear combination of vectors in $\beta$. 
\end{mytheorem}

\begin{mytheorem} If a vector space $V$ is generated by a finite set $S$, then some subset of $S$ is a basis for $V$. Hence $V$ has a finite basis.
\end{mytheorem}

\begin{mytheorem} (Replacement Theorem) Let $V$ be a vector space that is generated by a set $G$ containing exactly $n$ vectors, and let $L$ be an independent subset of $V$ containing exactly $m$ vectors. Then $m \leq n$ and there exists a subset $H$ of $G$ containing exactly $n - m$ vectors such that $L \cup H$ generates $V$.
\end{mytheorem}

\begin{mytheorem} Let $V$ be a vector space having a finite basis. Then every basis for $V$ contains the same number of vectors. The number of vectors in such a basis is called the dimension of the vector space, and is denoted by $dim(V)$.
\end{mytheorem}

\begin{mydef} A vector space is called finite-dimensional if it has a finite basis. A vector space that is not finite-dimensional is said to be infinite-dimensional.
\end{mydef}

\begin{mytheorem} A vector space is infinite-dimensional if and only if it contains an infinite linearly independent subset.
\end{mytheorem}

\begin{mytheorem} Let $V$ be a vector space with dimension $n$. The following are all true:
\begin{enumerate}
\item Any finite spanning set for $V$ contains at least $n$ vectors, and any spanning set for $V$ that contains exactly $n$ vectors is a basis for $V$.
\item Any independent subset of $V$ contains at most $n$ vectors, and any independent subset of $V$ that contains exactly $n$ vectors is a basis for $V$.
\item Every independent subset of $V$ can be extended to a basis for $V$ (via the Replacement Theorem).
\item Every spanning set for $V$ can be reduced to a basis for $V$, that is, if $span(S)=V$, then some subset of $S$ is a basis for$V$.
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} Let $V$ be a vector space with dimension $n$ and let $S$ be a subset of $V$. If any two of the following three statements are true, the third statement must also be true:
\begin{enumerate}
\item $S$ is linearly independent.
\item $span(S) = V$.
\item $S$ contains $n$ vectors.
\end{enumerate}
\end{mytheorem}


\begin{mytheorem} Let $W$ be a subspace of a finite-dimensional vector space $V$. Then $W$ is finite-dimensional and $dim(W) \leq dim(V)$. Moreover, if $dim(W) = dim(V)$, then $V = W$.
\end{mytheorem}

\begin{mytheorem} If $W$ is a subspace of a finite-dimensional vector space $V$, then any basis for $W$ can be extended to a basis for $V$.
\end{mytheorem}

\begin{example} If $W_{1},W_{2}$ are $m$ and $n$ dimensional subspaces respectively of a vector space $V$, where $m \geq n$, then $dim(W_{1} \cap W_{2}) \leq n$, and $dim(W_{1} + W_{2}) \leq m + n$.
\end{example}

\begin{example} If $W_{1},W_{2}$ are subspaces of a vector space $V$ such that $V = W_{1} \bigoplus W_{2}$, and if $\beta_{1},\beta_{2}$ are bases for $W_{1},W_{2}$ respectively, then $\beta_{1} \cap \beta_{2} = \varnothing$ and $\beta_{1} \cup \beta_{2}$ is a basis for $V$. Conversely, if $\beta_{1},\beta_{2}$ are disjoint bases for the subspaces $W_{1},W_{2}$ of the vector space $V$, then if $\beta_{1} \cup \beta_{2}$ is a basis for $V$, then $V = W_{1} \bigoplus W_{2}$.
\end{example}


\begin{mydef} Let $c_{0},c_{1},...,c_{n}$ be distinct vectors in an infinite field $F$. The polynomials $f_{0}(x),f_{1}(x),...,f_{n}(x)$ defined by $$f_{i}(x) = \frac{(x-c_{0})...(x-c_{i-1})(x-c_{i+1})...(x-c_{n})}{(c_{i}-c_{0})...(c_{i}-c_{i-1})(c_{i}-c_{i+1})...(c_{i}-c_{n})}$$ are called the Lagrange polynomials associated with the scalars $c_{0},c_{1},...,c_{n}$.
\end{mydef}

\begin{mytheorem} The polynomials $f_{0}(x),f_{1}(x),...,f_{n}(x)$ as defined above form a basis for $P_{n}(F)$. Any polynomial $g \in P_{n}(F)$ can be represented uniquely as the linear combination $g = \sum^{n}_{i=0}g(c_{i})f_{i}$.
\end{mytheorem}

\begin{mytheorem} Given a set of $n+1$ points $(c_{0},b_{0}),(c_{1},b_{1}),...,(c_{n},b_{n}) \in F^{2}$ (where $F$ is an infinite field) with distinct $c_{0},c_{1},..,c_{n}$, the Lagrange interpolation of these points is the polynomial $g = \sum^{n}_{i= 0}b_{i}f_{i}$ where $f_{0}(x),f_{1}(x),...,f_{n}(x)$ are the Lagrange polynomials associated with $c_{0},c_{1},...,c_{n}$. Note that $g(c_{i}) = b_{i}$ for all $i = 0,1,2,..,n$, meaning that $g$ passes through the given $n+1$ points, and that there is no other polynomial in $P_{n}(F)$ that also passes through all of these $n+1$ points.
\end{mytheorem}

\begin{example} If $W_{1},W_{2}$ are finite-dimensional subspaces of a vector space $V$, then $$dim(W_{1} \cap W_{2}) + dim(W_{1} + W_{2}) = dim(W_{1}) + dim(W_{2})$$
\end{example}

\subsection{Maximal linearly independent subsets}

\begin{mydef} Let $\mathcal{F}$ be a collection of sets. A member $M$ of $\mathcal{F}$ is called maximal (with respect to set inclusion) if $M$ is contained in no member of $\mathcal{F}$ other than $M$ itself.
\end{mydef}

\begin{mydef} A collection of sets $\mathcal{C}$ is called a chain (or nest or tower) if for each pair of sets $A$ and $B$ in $\mathcal{C}$, either $A \subseteq B$ or $B \subseteq A$. 
\end{mydef}

\begin{mytheorem} Given a chain $\mathcal{C}$, and a finite collection of sets $A_{1},...,A_{n} \in \mathcal{C}$, one of these sets contains all the others; that is, there is some $i \in \{ 1,2,...,n \} $ such that $A_{k} \subseteq A_{i}$ for $k = 1,2,..,n$.
\end{mytheorem}

\begin{axiom} (Maximal Principle) Let $\mathcal{F}$ be a family of sets. If, for each chain $\mathcal{C} \subseteq F$, there exists a member of $\mathcal{F}$ that contains each member of $\mathcal{C}$, then $\mathcal{F}$ contains a maximal member.
\end{axiom}

\begin{mydef} Let $S$ be a subset of a vector space $V$. A maximal linearly independent subset of $S$ is a subset $B$ of $S$ satisfying both of the following conditions:
\begin{enumerate}
\item B is linearly independent.
\item The only linearly independent subset of $S$ that contains $B$ is $B$ itself.
\end{enumerate}
\end{mydef}

\begin{mytheorem} Let $V$ be a vector space and $S$ a subset that generates $V$. If $\beta$ is a maximal linearly independent subset of $S$ if and only if $\beta$ is a basis for $V$.
\end{mytheorem}

\begin{mytheorem} Let $S$ be a linearly independent subset of a vector space $V$. There exists a maximal linearly independent subset of $V$ that contains $S$.
\end{mytheorem}

\begin{mytheorem} Every vector space has a basis.
\end{mytheorem}

\begin{example} If $W$ is a subspace of a vector space $V$, then every basis for $W$ is a subset of some basis for $V$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Transformations and Matrices}

\subsection{Linear transformations, null spaces, and ranges}

\begin{mydef} Let $V$ and $W$ be vector spaces (over $F$). We call a function $T: V \rightarrow W$ a linear transformation from $V$ to $W$ if, for all $x,y \in V$, and $c \in F$, we have:
\begin{enumerate}
\item $T(x+y) = T(x) + T(y)$
\item $T(cx) = cT(x)$
\end{enumerate}
\end{mydef}

\begin{mytheorem} Properties of linear transformations:
\begin{enumerate}
\item If $T$ is linear, $T(0) = 0$.
\item $T$ is linear if and only if $T(cx+y) = cT(x) + T(y)$ for all $x,y \in V$, and $c \in F$.
\item If $T$ is linear, then $T(x-y) = T(x)-T(y)$ for all $x,y \in V$
\item $T$ is linear if and only if for all $x_{1},x_{2},...,x_{n} \in V$ and $a_{1},a_{2},...,a_{n} \in F$, we have $$T(\sum^{n}_{i=1}a_{i}x_{i}) = \sum^{n}_{i=1}a_{i}T(x_{i})$$.
\end{enumerate}
\end{mytheorem}

\begin{example} The transformation $T_{\theta}: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ defined by $T_{\theta}(a_{1},a_{2}) = (a_{1}cos \theta - a_{2} sin \theta, a_{1} sin \theta + a_{2} cos \theta)$ is a linear transformation, and is called rotation by $\theta$. 
\end{example}

\begin{example} The transformation $T: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ defined by $T(a_{1},a_{2}) = (a_{1},-a_{2})$ is a linear transformation, and is called reflection about the x-axis. Reflection about the y-axis is defined similarly, and is also a linear transformation.
\end{example}

\begin{example} The transformation $T: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ defined by $T(a_{1},a_{2}) = (a_{1},0)$ is a linear transformation, and is called projection on the x-axis. Projection on the y-axis is defined similarly, and is also a linear transformation.
\end{example}

\begin{mydef} The transformation $I_{V}: V \rightarrow V$ defined by $I_{V}(x) = x$ for all $x \in V$ is called the identity transformation in $V$. The transformation $T_{0}: V \rightarrow W$ defined by $T_{0}(x) = 0$ for all $x \in V$ is called the zero transformation from $V$ to $W$. Both of these are linear transformations, and $I_{V}$ is often simply written as $I$.
\end{mydef}

\begin{mydef} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear. The null space (or kernel) of $T$, denoted as $N(T)$, is the subset of $V$ that is mapped by $T$ onto $0 \in W$. 
$$ N(T) = \{ v \in V | T(v) = 0 \} $$
The range of $T$, denoted by $R(T)$, is the subset of $W$ that is mapped onto by some vector in $V$. 
$$ R(T) = \{ w \in W | \exists v \in V : T(v) = w \} $$
\end{mydef}

\begin{mytheorem} Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. Then $N(T)$ and $R(T)$ are subspaces of $V$ and $W$, respectively.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. If $\beta = \{v_{1},v_{2},...,v_{n} \}$ is a basis for $V$, then $R(T) = span(T(\beta)) = span(\{ T(v_{1}),T(v_{2}),...,T(v_{n}) \} )$.
\end{mytheorem}

\begin{mydef}  Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. If $N(T)$ and $R(T)$ are finite-dimensional, then we define the nullity of $T$, denoted nullity(T), and the rank of T, denoted rank(T), to be the dimensions of $N(T)$ and $R(T)$, respectively.
\end{mydef}

\begin{mytheorem} (Dimension Theorem/Rank Nullity Theorem) Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. If V is finite-dimensional, then $$ nullity(T) + rank(T) = dim(V) $$.
\end{mytheorem}

\begin{mydef}  A function $f : S \rightarrow T$ is said to be one-to-one (or injective) if $f(x) = f(y)$ implies that $x = y$. Such a function is said to be an injection.
\end{mydef}

\begin{mydef}  A function $f : S \rightarrow T$ is said to be onto (or surjective) if its range is equal to $T$. $T = \{ y \in T | \exists x \in S : f(x) = y \} $. Such a function is said to be a surjection.
\end{mydef}

\begin{mytheorem} Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. $T$ is one-to-one if and only if $N(T) = \{ 0 \}$.
\end{mytheorem}

\begin{mytheorem}Let $V$ and $W$ be vector spaces of equal (finite) dimension, and let $T: V \rightarrow W$ be linear. The following statements are equivalent (all true or all false):
\begin{enumerate}
\item $T$ is one-to-one.
\item $T$ is onto.
\item rank(T) = dim(V).
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be vector spaces over $F$ and suppose that $\{ v_{1},v_{2},...,v_{n} \} $ is a basis for $V$. For vectors $w_{1},w_{2},...,w_{n}$ in $W$, there exists exactly one linear transformation $T: V \rightarrow W$ such that $T(v_{i}) = w_{i}$ for $i = 1,2,..,n$.
\end{mytheorem}

\begin{mytheorem} (Transformations that agree on a basis are equivalent) Let $V$ and $W$ be vector spaces over $F$ and suppose that $\{ v_{1},v_{2},...,v_{n} \} $ is a basis for $V$. If $U,T: V \rightarrow W$ are both linear, and $U(v_{i}) = T(v_{i})$ for $i = 1,2,...,n$, then $U = T$. 
\end{mytheorem}

\begin{example} Let $V$ and $W$ be vector spaces and $T: V \rightarrow W$ be linear. 
\begin{enumerate}
\item $T$ is one-to-one if and only if $T$ carries linearly independent subsets of $V$ onto linear independent subsets of $W$.
\item Suppose that $T$ is one-to-one and that $S$ is a subset of $V$. Then $S$ is linearly independent if and only if $T(S)$ is linearly independent.
\item Suppose $\beta$ is a basis for $V$ and $T$ is one-to-one and onto. Then $T(\beta)$ is a basis for $W$.
\end{enumerate}
\end{example}

\begin{example} Let $V$ and $W$ be finite-dimensional vector spaces and $T: V \rightarrow W$ be linear. 
\begin{enumerate}
\item If $T$ is onto, then $dim(V) \geq dim(W)$.
\item If $T$ is one-to-one, then $dim(V) \leq dim(W)$.
\end{enumerate}
\end{example}

\begin{example} Let $V$ and $W$ be vector spaces with subspaces $V_{1}$ and $W_{1}$, respectively. If $T: V \rightarrow W$ is linear, then $T(V_{1})$ is a subspace of $W$ and  $\{ x \in V: T(x) \in W_{1} \}$ is a subspace of $V$. 
\end{example}

\begin{mydef}  Let $V$ be a vector space and $W_{1}$ and $W_{2}$ be subspaces of $V$ such that $V = W_{1} \bigoplus W_{2}$. A function $T: V \rightarrow V$ is called the projection on $W_{1}$ along $W_{2}$ if, for $x = x_{1} + x_{2}$ with $x_{1} \in W_{1}, x_{2} \in W_{2}$, we have $T(x) = x_{1}$.
\end{mydef}

\begin{example} Using the notation in the definition above, assume that $T: V \rightarrow V$ is the projection on $W_{1}$ along $W_{2}$:
\begin{enumerate} 
\item $T$ is  linear, and $W_{1} = \{ x \in V : T(x) = x \}$.
\item $W_{1} = R(T)$ and $W_{2} = N(T)$.
\end{enumerate} 
\end{example}

\begin{mydef}  Let $V$ be a vector space and let $T: V \rightarrow V$ be linear. A subspace $W$ of $V$ is said to be T-invariant if $T(x) \in W$ for all $x \in W$, i.e. $T(W) \subseteq W$. If $W$ is T-invariant, we define the restriction of $T$ on $W$ to be the function $T_{W}: W \rightarrow W$ defined by $T_{W}(x) = T(x)$ for all $x \in W$. 
\end{mydef}

\begin{example} If $W$ is a subspace of a vector space $V$, and $T: V \rightarrow V$ is linear, then the following are all true:
\begin{enumerate} 
\item $\{ 0 \}$, $V$, $R(T)$ and $N(T)$ are all T-invariant.
\item If $W$ is T-invariant, then $T_{W}$ is linear.
\item If $T$ is the projection on $W$ along some subspace $W'$, then $W$ is T-invariant and $T_{W} = I_{W}$.
\item If $V = R(T) \bigoplus W$ and $W$ is T-invariant, then $W \subseteq N(T)$. Furthermore, if $V$ is finite-dimensional, then $W = N(T)$. 
\item If $W$ is T-invariant, then $N(T_{W}) = N(T) \cap W$ and $R(T_{W}) = T(W)$.
\end{enumerate} 
\end{example}

\begin{example} Let $V$ be finite-dimensional vector space and $T: V \rightarrow V$ be linear. 
\begin{enumerate} 
\item If $V = R(T) + N(T)$, then $V = R(T) \bigoplus N(T)$. 
\item If $R(T) \cap N(T) = \{ 0 \}$, then $V = R(T) \bigoplus N(T)$. 
\end{enumerate} 
\end{example}

\subsection{Matrix representation of a linear transformation}

\begin{mydef}  Let $V$ be a finite-dimensional vector space. An ordered basis for $V$ is a basis for $V$ endowed with a specific order; that is, an ordered basis for $V$ is a finite sequence of linearly independent vectors in $V$ that generates $V$.
\end{mydef}

\begin{mydef}  For the vector space $F^{n}$, we call $\{ e_{1},e_{2},...,e_{n}\}$ the standard basis for $F^{n}$. Similarly, for the vector space $P_{n}(F)$, we call $\{ 1, x, ..., x^{n} \} $ the standard ordered basis for $P_{n}(F)$. The vectors of the standard basis are called the standard vectors.
\end{mydef}

\begin{mydef}  Let $\beta = \{u_{1},u_{2},...,u_{n} \} $ be an ordered basis for a finite-dimensional vector space $V$. For $x \in V$, let $a_{1},a_{2},..,a_{n}$ be the unique scalars such that $x = \sum^{n}_{i = 1} a_{i}u_{i}$. We define the coordinate vector of $x$ relative to $\beta$, denoted $[x]_{\beta}$, by $[x]_{\beta} = (a_{1},a_{2},..,a_{n})$. 
\end{mydef}

\begin{mydef}  Let $V,W$ be finite-dimensional vector spaces and let $T: V \rightarrow W$ be linear. If $\beta = \{ v_{1},v_{2},..,v_{n} \}$ and $\gamma = \{w_{1},w_{2},...,w_{m} \}$ are ordered bases for $V,W$ respectively, then for each $j, 1 \leq j \leq n$ there exist unique scalars $a_{ij} \in F, 1 \leq i \leq m$, such that $$T(v_{j}) = \sum^{m}_{i = 1} a_{ij}w_{i}, 1 \leq j \leq n$$ The $m \times n$ matrix $A$ defined by $A_{ij} = a_{ij}$ is called the matrix representation of $T$ in the ordered bases $\beta$ and $\gamma$ and write $A = [T]^{\gamma}_{\beta}$. If $V = W$ and $\beta = \gamma$, we write $A = [T]_{\beta}$.
\end{mydef}

\begin{mydef}  Let $T,U: V \rightarrow W$ be arbitrary functions, where $V,W$ are vector spaces over $F$, and let $a \in F$. Define $T + U: V \rightarrow W$ by $(T+U)(x) = T(x) + U(x)$ for all $x \in V$, and $aT: V \rightarrow W$ by $(aT)(x) = aT(x)$ for all $x \in V$.
\end{mydef}

\begin{mytheorem}Let $V$ and $W$ be vector spaces over a field $F$, and let $T,U: V \rightarrow W$ be linear. Then:
\begin{enumerate} 
\item For all $a \in F$, $aT + U$ is linear. 
\item Using the operations of addition and scalar multiplication in the preceding definition, the collection of linear transformations from $V$ to $W$ is a vector space over $F$.
\end{enumerate} 
\end{mytheorem}

\begin{mydef}  Let $V,W$ be vector spaces over $F$. The vector space of all linear transformations from $V$ to $W$ is denoted as $\mathcal{L}(V,W)$. In the case that $V = W$, we simply write $\mathcal{L}(V)$ instead of $\mathcal{L}(V,W)$.
\end{mydef}

\begin{mytheorem}Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $\beta$ and $\gamma$ respectively, and let $T,U: V \rightarrow W$ be linear transformations. Then:
\begin{enumerate} 
\item $[T+U]^{\gamma}_{\beta} = [T]^{\gamma}_{\beta} + [U]^{\gamma}_{\beta}$
\item $[aT]^{\gamma}_{\beta} = a[T]^{\gamma}_{\beta}$ for all $a \in F$
\end{enumerate}
\end{mytheorem}

\subsection{Composition of linear transformations and matrix multiplication}

\begin{mytheorem}Let $V,W,Z$ be vector spaces over the same field $F$, and let $T: V \rightarrow W$ and $U: W \rightarrow Z$ be linear. Then $UT: V \rightarrow Z$ is linear. 
\end{mytheorem}

\begin{mytheorem}Let $V$ be a vector space. Let $T,U_{1},U_{2} \in \mathcal{L}(V)$. Then:
\begin{enumerate} 
\item $T(U_{1}+U_{2}) = TU_{1} + TU_{2}$ and $(U_{1}+U_{2})T = U_{1}T + U_{2}T$
\item $T(U_{1}U_{2}) = (TU_{1})U{2}$
\item $TI = IT = T$
\item $a(U_{1}U_{2}) = (aU_{1})U_{2} = U_{1}(aU_{2})$ for all scalars $a \in F$.
\end{enumerate}
\end{mytheorem}

\begin{mydef}  Let $A$ be an $m \times n$ matrix and $B$ be an $n \times p$ matrix. We define the product of $A$ and $B$, denoted $AB$, to be the $m \times p$ matrix such that $$(AB)_{ij} = \sum^{n}_{k = 1}A_{ik}B_{kj}, 1 \leq i \leq m, 1 \leq j \leq p$$
\end{mydef}

\begin{mytheorem}Let $V,W,Z$ be finite-dimensional vector spaces over the same field $F$, with ordered bases $\alpha,\beta,\gamma$ respectively, and let $T: V \rightarrow W$ and $U: W \rightarrow Z$ be linear. Then $[UT]^{\gamma}_{\alpha} = [U]^{\gamma}_{\beta}[T]^{\beta}_{\gamma}$. 
\end{mytheorem}

\begin{mytheorem}Let $V$ be a finite-dimensional vector space with an ordered basis $\beta$. Let $T,U \in \mathcal{L}(V)$. Then $[UT]_{\beta} = [U]_{\beta}[T]_{\beta}$.
\end{mytheorem}

\begin{mydef} We define the Kronecker delta $\delta_{ij}$ by $\delta_{ij} = 1$ if $i = j$ and $\delta_{ij} = 0$ if $i \neq j$. The $n \times n$ identity matrix $I_{n}$, is defined by $(I_{n})_{ij} = \delta_{ij}$.
\end{mydef}

\begin{mytheorem}Let $A$ be an $m \times n$ matrix, $B$ and $C$ be $n \times p$ matrices, and $D$ and $E$ be $q \times m$ matrices, all with entries from a field $F$. Then:
\begin{enumerate} 
\item $A(B+C) = AB + AC$ and $(D + E)A = DA + EA$.
\item $a(AB) = (aA)B = A(aB)$ for all scalars $a \in F$.
\item $I_{m}A = A = AI_{n}$
\item If $V$ is an n-dimensional vector space with an ordered basis $\beta$, then $[I_{V}]_{\beta} = I_{n}$.
\end{enumerate}
\end{mytheorem}

\begin{mytheorem}Let $A$ be an $m \times n$ matrix and $B$ be an $n \times p$ matrix. For each $1 \leq j \leq p$, let $u_{j}$ and $v_{j}$ denote the $jth$ columns of $AB$ and $B$, respectively. Then:
\begin{enumerate} 
\item $u_{j} = Av_{j}$
\item $v_{j} = Be_{j}$ where $e_{j}$ is the $jth$ standard vector of $F^{p}$.
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be finite-dimensional vector spaces having ordered bases $\beta$ and $\gamma$ respectively and let $T: V \rightarrow W$ be linear. Then, for each $u \in V$, we have $$[T(u)]_{\gamma} = [T]^{\gamma}_{\beta}[u]_{\beta}$$. 
\end{mytheorem}

\begin{mydef} Let $A$ be an $m \times n$ matrix with entries from a field $F$. We denote by $L_{A}$ the mapping $L_{A} : F^{n} \rightarrow F^{m}$ denited by $L_{A}(x) = Ax$ for each column vector $x \in F^{n}$. We call $L_{A}$ a left-multiplication transformation.
\end{mydef}

\begin{mytheorem}Let $A$ be an $m \times n$ matrix with entries from a field $F$. Then the left-multiplication transformation $L_{A}: F^{n} \rightarrow F^{m}$ is linear. Furthermore, if $B$ is any other $m \times n$ matrix (with entries from $F$) and $\beta$ and $\gamma$ are the standard ordered bases for $F^{n}$ and $F^{m}$ respectively, then we have the following:
\begin{enumerate} 
\item $[L_{A}]^{\gamma}_{\beta} = A$
\item $L_{A} = L_{B}$ if and only if $A = B$.
\item $L_{A+B} = L_{A} + L_{B}$ and $L_{aA} = aL_{A}$ for all $a \in F$.
\item If $T: F^{n} \rightarrow F^{m}$ is linear, then there exists a unique $m \times n$ matrix $C$ such that $T = L_{C}$. In fact, $C = [T]^{\gamma}_{\beta}$.
\item If $E$ is an $n \times p$ matrix, then $L_{AE} = L_{A}L_{E}$.
\item If $m = n$, then $L_{I_{n}} = I_{F^{n}}$.
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} Let $A,B,C$ be matrices such that $A(BC)$ is defined. Then $(AB)C$ is also defined and $A(BC) = (AB)C$. That is, matrix multiplication is associative.
\end{mytheorem}

\begin{example} Let $M$ and $A$ be matrices for which the product $MA$ is defined. If the $jth$ column of $A$ is a linear combination of a set of columns of $A$, then the $jth$ column of $MA$ is a linear combination of the corresponding columns of $MA$ with the same corresponding coefficients.
\end{example}

\subsection{Invertibility and isomorphisms}

\begin{mydef} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear. A function $U: V \rightarrow W$ is said to be an inverse of $T$ if $TU = I_{W}$ and $UT = I_{V}$. If $T$ has an inverse, then $T$ is said to be invertible, and its inverse is denoted by $T^{-1}$.
\end{mydef}

\begin{mytheorem} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear. If $T$ is invertible, its inverse is unique.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ and $U: V \rightarrow W$ be linear. If $T$ and $U$ are invertible, then:
\begin{enumerate} 
\item $(TU)^{-1} = U^{-1}T^{-1}$
\item $T^{-1}$ is also invertible, and in particular, $(T^{-1})^{-1} = T$. 
\end{enumerate}
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear. Then $T$ is invertible if and only if it is both one-to-one and onto. Also, $T$ is invertible if and only if $rank(T) = dim(V)$.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear and invertible. Then $T^{-1}$ is linear.
\end{mytheorem}

\begin{mydef} Let $A$be an $n \times n$ matrix. Then $A$ is invertible if there exists an $n \times n$ matrix $B$ such that $AB = BA = I$.
\end{mydef}

\begin{mytheorem} Let $V$ and $W$ be vector spaces, and let $T: V \rightarrow W$ be linear and invertible. Then $V$ is finite-dimensional if and only if $W$ is finite-dimensional. In this case, $dim(V) = dim(W)$.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $\beta$ and $\gamma$, respectively. Let $T: V \rightarrow W$ be linear. Then $T$ is invertible if and only if $[T]^{\gamma}_{\beta}$ is invertible. Furthermore, $[T^{-1}]^{\beta}_{\gamma} = ([T]^{\gamma}_{\beta})^{-1}$. 
\end{mytheorem}

\begin{mytheorem} Let $V$ be a finite-dimensional vector space with an ordered basis $\beta$. Let $T: V \rightarrow V$ be linear. Then $T$ is invertible if and only if $[T]_{\beta}$ is invertible. Furthermore, $[T^{-1}]^{\beta}= ([T]^{\gamma}_{\beta})^{-1}$. 
\end{mytheorem}

\begin{mytheorem} Let $A$ be an $n \times n$ matrix. Then $A$ is invertible if and only if $L_{A}$ is invertible. Furthermore, $(L_{A})^{-1} = L_{A^{-1}}$. 
\end{mytheorem}

\begin{mydef} Let $V$ and $W$ be vector spaces. We say that $V$ is isomorphic to $W$ if there exists a linear transformation $T: V \rightarrow W$ that is invertible. Such a linear transformation is called an isomorphism from $V$ onto $W$.
\end{mydef}

\begin{mytheorem} Let $V$ and $W$ be finite-dimensional vector spaces (over the same field). Then $V$ is isomorphic to $W$ if and only if $dim(V) = dim(W)$.
\end{mytheorem}

\begin{mytheorem} Let $V$ be a vector space over a field $F$. Then $V$ is isomorphic to $F^{n}$ if and only if $dim(V) = n$.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be finite-dimensional vector spaces over $F$ of dimensions $n$ and $m$, respectively, and let $\beta$ and $\gamma$ be ordered bases for $V$ and $W$, respectively. Then the function $\phi: \mathcal{L}(V,W) \rightarrow M_{m \times n}(F)$ defined by $\Phi(T) = [T]^{\gamma}_{\beta}$ is an isomorphism.
\end{mytheorem}

\begin{mytheorem} Let $V$ and $W$ be finite-dimensional vector spaces over $F$ of dimensions $n$ and $m$, respectively. Then $\mathcal{L}(V,W)$ is finite-dimensional of dimension $mn$.
\end{mytheorem}

\begin{mydef} Let $\beta$ be an ordered basis for an n-dimensional vector space $V$ over the field $F$. The standard representation of $V$ with respect to $\beta$ is the function $\phi_{\beta}: V \rightarrow F^{n}$ defined by $\phi_{\beta}(x) = [x]_{\beta}$ for each $x \in V$. 
\end{mydef}

\begin{mytheorem} For any finite-dimensional vector space $V$ with ordered basis $\beta$, $\phi_{\beta}$ is an isomorphism.
\end{mytheorem}

\begin{example} Let $V$ and $W$ be finite-dimensional vector spaces, and let $T: V \rightarrow W$ be an ismorphism. Let $V_{0}$ be a subspace of $V$. Then:
\begin{enumerate} 
\item $T(V_{0})$ is a subspace of $W$.
\item $dim(V_{0}) = dim(T(V_{0}))$.
\end{enumerate}
\end{example}

\subsection{The change of coordinate matrix}

\begin{mytheorem} Let $\beta$ and $\beta'$ be two ordered bases for a finite-dimensional vector space $V$, and let $Q = [I_{V}]^{\beta}_{\beta'}$. Then:
\begin{enumerate} 
\item $Q$ is invertible.
\item For any $v \in V$, $[v]_{\beta} = Q[v]_{\beta'}$.
\end{enumerate}
\end{mytheorem}

\begin{mydef} Using the notation above, the matrix $Q = [I_{V}]^{\beta}_{\beta'}$ is called the change of coordinate matrix that changes $\beta'$ coordinates into $\beta$ coordinates.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
              
